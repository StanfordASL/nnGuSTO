{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# organize imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import normalize\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pydot\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for reproducing same results\n",
    "seed = 9\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and create test and training inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames must be a list\n",
    "def loadData(filenames, validationRatio = 0.10, testRatio = 0.25, fieldNames = [\"train_input_collected\", \"train_output_collected\"]):\n",
    "    # load simple dataset\n",
    "    for i, filename in enumerate(filenames):\n",
    "        PATH_FILE_TRAINING3 = os.getcwd()+\"/\"+filename\n",
    "        f = h5py.File(PATH_FILE_TRAINING3, \"r\")\n",
    "        # split into input and output variables\n",
    "        if i==0:\n",
    "            X       = f[fieldNames[0]].value;     \n",
    "            X      = X.transpose();\n",
    "            Y        = f[fieldNames[1]].value;      \n",
    "            Y       = Y.transpose();\n",
    "        else:\n",
    "            X1       = f[fieldNames[0]].value;     \n",
    "            X1      = X1.transpose();\n",
    "            Y1        = f[fieldNames[1]].value;      \n",
    "            Y1       = Y1.transpose();\n",
    "            X = np.vstack((X,X1))\n",
    "            Y = np.vstack((Y,Y1))   \n",
    "    # Take out some data (10%) to save for validation\n",
    "    (X_rest, X_valid, Y_rest, Y_valid) = train_test_split(X, Y, test_size=validationRatio, random_state=seed)\n",
    "    # split the remaining data into training (75%) and testing (25%)\n",
    "    (X_train, X_test, Y_train, Y_test) = train_test_split(X_rest, Y_rest, test_size=testRatio, random_state=seed)\n",
    "\n",
    "    # Examine shape of data\n",
    "#     print(np.shape(X))\n",
    "#     print(np.shape(Y))\n",
    "#     print(np.shape(X_valid))\n",
    "#     print(np.shape(Y_valid))\n",
    "#     print(np.shape(X_train))\n",
    "#     print(np.shape(X_test))\n",
    "#     print(np.shape(Y_train))\n",
    "#     print(np.shape(Y_test))\n",
    "    return X_train, X_test, X_valid, Y_train, Y_test, Y_valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveSplitDataValidTrainTest(filename, X_train_norm, X_test_norm, X_valid_norm, Y_train_norm, Y_test_norm, Y_valid_norm):\n",
    "    # Save these into an h5 file\n",
    "    hf = h5py.File(filename, 'w')\n",
    "    hf.create_dataset(\"X_train_norm\", data = X_train_norm)\n",
    "    hf.create_dataset(\"X_test_norm\", data = X_test_norm)\n",
    "    hf.create_dataset(\"X_valid_norm\", data = X_valid_norm)\n",
    "    hf.create_dataset(\"Y_train_norm\", data = Y_train_norm)\n",
    "    hf.create_dataset(\"Y_test_norm\", data = Y_test_norm)\n",
    "    hf.create_dataset(\"Y_valid_norm\", data = Y_valid_norm)\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSplitDataValidTrainTest(filename):\n",
    "    hf = h5py.File(filename, \"r\")\n",
    "    X_train_norm = np.array(hf.get(\"X_train_norm\"))\n",
    "    X_test_norm = np.array(hf.get(\"X_test_norm\"))\n",
    "    X_valid_norm = np.array(hf.get(\"X_valid_norm\"))\n",
    "    Y_train_norm = np.array(hf.get(\"Y_train_norm\"))\n",
    "    Y_test_norm = np.array(hf.get(\"Y_test_norm\"))\n",
    "    Y_valid_norm = np.array(hf.get(\"Y_valid_norm\"))\n",
    "    hf.close() \n",
    "    return X_train_norm, X_test_norm, X_valid_norm, Y_train_norm, Y_test_norm, Y_valid_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom metric to print learning rate after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMetrics(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_lr = K.eval(self.model.optimizer.lr)\n",
    "        lrHist.append(current_lr)\n",
    "        print(\"Learning rate: \",current_lr)\n",
    "#     def on_batch_end(self, batch, logs):\n",
    "#         weights, _biases = model.get_weights()\n",
    "#         w1, w2 = weights\n",
    "#         weights = [w1[0], w2[0]]\n",
    "#         print('on_batch_end() model.weights:', weights)\n",
    "#         weights_history.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCoeffsExpectedAndNN(Y, NN):\n",
    "    # Y and NN have same shape (1,*) \n",
    "    xs = range(1,np.size(Y,0)+1)\n",
    "    plt.scatter(xs, Y, color='r',marker='s', s = 30)\n",
    "    plt.scatter(xs, NN, color='g', marker='^', s = 20)\n",
    "    plt.title('Poly coeffs deviation from expected')\n",
    "    plt.ylabel('Value of coeff')\n",
    "    plt.xlabel('Coefficient number')\n",
    "    plt.legend(['true coeffs', 'NN output coeffs'], loc='lower left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_datasets(X_train, X_test, X_valid, Y_train, Y_test, Y_valid):\n",
    "    # Normalize all datasets to be between [0,1]\n",
    "#     xMin = np.amin(X, axis=0)\n",
    "    xMin = np.array([0.0,0.0,0.5,   0.,0.,0.,  0.,0.,0., 1.,  0.,0.,0., \n",
    "           0.0,0.0,0.5,   0.,0.,0.,  0.,0.,1., 0.,  0.,0.,0.])\n",
    "#     xMax = np.amax(X, axis=0)\n",
    "    xMax = np.array([10.0,10.0,0.5,   0.,0.,0.,  0.,0.,0., 1.,  0.,0.,0., \n",
    "           10.0,10.0,0.5,   0.,0.,0.,  0.,0.,1., 0.,  0.,0.,0.])\n",
    "    xRange = xMax - xMin\n",
    "    \n",
    "    xRange[xRange == 0] = 1.0\n",
    "    xRange[abs(xRange) <1.0e-20]=1.0\n",
    "    X_train_norm = (X_train-xMin)/xRange\n",
    "    X_test_norm = (X_test-xMin)/xRange\n",
    "    X_valid_norm = (X_valid-xMin)/xRange\n",
    "\n",
    "    Y = np.vstack([Y_train, Y_test, Y_valid])\n",
    "    yMin = np.amin(Y, axis=0)\n",
    "    yMax = np.amax(Y, axis=0)\n",
    "    yRange = yMax - yMin\n",
    "    yRange[yRange == 0] = 1.0\n",
    "    yRange[abs(yRange) <1.0e-20]=1.0\n",
    "    Y_train_norm = (Y_train-yMin)/yRange\n",
    "    Y_test_norm = (Y_test-yMin)/yRange\n",
    "    Y_valid_norm = (Y_valid-yMin)/yRange\n",
    "    \n",
    "    return (X_train_norm, Y_train_norm,X_test_norm, Y_test_norm,X_valid_norm, Y_valid_norm, xMin, xRange, yMin, yRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataParamsFromHDF5Old(filename):\n",
    "    hf = h5py.File(filename, \"r\")\n",
    "    xMin_train = np.array(hf.get(\"xMin_train\"))\n",
    "    yMin_train = np.array(hf.get(\"yMin_train\"))\n",
    "    xRange_train = np.array(hf.get(\"xRange_train\"))\n",
    "    yRange_train = np.array(hf.get(\"yRange_train\"))\n",
    "    xMin_test = np.array(hf.get(\"xMin_test\"))\n",
    "    yMin_test = np.array(hf.get(\"yMin_test\"))\n",
    "    xRange_test = np.array(hf.get(\"xRange_test\"))\n",
    "    yRange_test = np.array(hf.get(\"yRange_test\"))\n",
    "    xMin_valid = np.array(hf.get(\"xMin_valid\"))\n",
    "    yMin_valid = np.array(hf.get(\"yMin_valid\"))\n",
    "    xRange_valid = np.array(hf.get(\"xRange_valid\"))\n",
    "    yRange_valid = np.array(hf.get(\"yRange_valid\"))\n",
    "    hf.close() \n",
    "    return xMin_train, yMin_train, xRange_train, yRange_train, xMin_test, yMin_test, xRange_test, yRange_test, xMin_valid, yMin_valid, xRange_valid, yRange_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDataParamsToHDF5(filename, xMin, yMin, xRange, yRange):\n",
    "    # Save these params into an h5 file\n",
    "    hf = h5py.File(filename, 'w')\n",
    "    hf.create_dataset(\"xMin\", data = xMin)\n",
    "    hf.create_dataset(\"yMin\", data = yMin)\n",
    "    hf.create_dataset(\"xRange\", data = xRange)\n",
    "    hf.create_dataset(\"yRange\", data = yRange)\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataParamsFromHDF5(filename):\n",
    "    hf = h5py.File(filename, \"r\")\n",
    "    xMin = np.array(hf.get(\"xMin\"))\n",
    "    yMin = np.array(hf.get(\"yMin\"))\n",
    "    xRange = np.array(hf.get(\"xRange\"))\n",
    "    yRange = np.array(hf.get(\"yRange\"))\n",
    "    hf.close() \n",
    "    return xMin, yMin, xRange, yRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDataParamsToHDF5Old(filename, xMin_train, yMin_train, xRange_train, yRange_train, xMin_test, yMin_test, xRange_test, yRange_test, xMin_valid, yMin_valid, xRange_valid, yRange_valid):\n",
    "    # Save these params into an h5 file\n",
    "    hf = h5py.File(filename, 'w')\n",
    "    hf.create_dataset(\"xMin_train\", data = xMin_train)\n",
    "    hf.create_dataset(\"yMin_train\", data = yMin_train)\n",
    "    hf.create_dataset(\"xRange_train\", data = xRange_train)\n",
    "    hf.create_dataset(\"yRange_train\", data = yRange_train)\n",
    "    hf.create_dataset(\"xMin_test\", data = xMin_test)\n",
    "    hf.create_dataset(\"yMin_test\", data = yMin_test)\n",
    "    hf.create_dataset(\"xRange_test\", data = xRange_test)\n",
    "    hf.create_dataset(\"yRange_test\", data = yRange_test)\n",
    "    hf.create_dataset(\"xMin_valid\", data = xMin_valid)\n",
    "    hf.create_dataset(\"yMin_valid\", data = yMin_valid)\n",
    "    hf.create_dataset(\"xRange_valid\", data = xRange_valid)\n",
    "    hf.create_dataset(\"yRange_valid\", data = yRange_valid)\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on the polynomial coeffs for polygonal obstacles for new astrobeeSE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1130, 26)\n",
      "(7625, 26)\n",
      "(2542, 26)\n",
      "(1130, 95)\n",
      "(7625, 95)\n",
      "(2542, 95)\n",
      "\n",
      "(1130, 26)\n",
      "(7625, 26)\n",
      "(2542, 26)\n",
      "(1130, 95)\n",
      "(7625, 95)\n",
      "(2542, 95)\n"
     ]
    }
   ],
   "source": [
    "(X_train, X_test, X_valid, Y_train, Y_test, Y_valid) = loadData([\"PolyAstrobeeCoeffs.jld2\", \"PolyMoreCoeffs.jld2\"], validationRatio = 0.1, testRatio = 0.25, fieldNames = [\"train_input_collected\", \"all_coeffs\"])\n",
    "# Each input is of size 26\n",
    "# Each output is of size 95\n",
    "print(np.shape(X_valid))\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(Y_valid))\n",
    "print(np.shape(Y_train))\n",
    "print(np.shape(Y_test))\n",
    "\n",
    "# X = np.vstack(X_valid, X_train, X_test)\n",
    "# Y = np.vstack(Y_valid, Y_train, Y_test)\n",
    "\n",
    "(X_train_norm, Y_train_norm,X_test_norm, Y_test_norm,X_valid_norm, Y_valid_norm, xMin, xRange, yMin, yRange) = normalize_datasets(X_train, X_test, X_valid, Y_train, Y_test, Y_valid)\n",
    "print(\"\")\n",
    "print(np.shape(X_valid_norm))\n",
    "print(np.shape(X_train_norm))\n",
    "print(np.shape(X_test_norm))\n",
    "print(np.shape(Y_valid_norm))\n",
    "print(np.shape(Y_train_norm))\n",
    "print(np.shape(Y_test_norm))\n",
    "\n",
    "\n",
    "filename = \"PolyComboCoeffsNormalized.jld2\"\n",
    "saveSplitDataValidTrainTest(filename, X_train_norm, X_test_norm, X_valid_norm, \n",
    "                            Y_train_norm, Y_test_norm, Y_valid_norm)\n",
    "\n",
    "filename = \"PolyComboParams.h5\"\n",
    "saveDataParamsToHDF5(filename, xMin, yMin, xRange, yRange)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=26, activation='relu'))\n",
    "model.add(Dense(256, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(512, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(256, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(95, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# compile the model\n",
    "customAdam = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='mean_squared_error', optimizer=customAdam, metrics=['mean_squared_error'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=2, min_lr=1e-50)\n",
    "\n",
    "lrHist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7625 samples, validate on 2542 samples\n",
      "Epoch 1/30\n",
      "7625/7625 [==============================] - 11s 1ms/step - loss: 0.0650 - mean_squared_error: 0.0650 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "('Learning rate: ', 0.001)\n",
      "Epoch 2/30\n",
      "7625/7625 [==============================] - 11s 1ms/step - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "('Learning rate: ', 0.001)\n",
      "Epoch 3/30\n",
      "7625/7625 [==============================] - 11s 1ms/step - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0295 - val_mean_squared_error: 0.0295\n",
      "('Learning rate: ', 0.001)\n",
      "Epoch 4/30\n",
      "7625/7625 [==============================] - 11s 1ms/step - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0214 - val_mean_squared_error: 0.0214\n",
      "('Learning rate: ', 0.001)\n",
      "Epoch 5/30\n",
      "7625/7625 [==============================] - 11s 1ms/step - loss: 0.0138 - mean_squared_error: 0.0138 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
      "('Learning rate: ', 0.001)\n",
      "Epoch 6/30\n",
      "3473/7625 [============>.................] - ETA: 5s - loss: 0.0062 - mean_squared_error: 0.0062"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "histSimple = model.fit(X_train_norm, Y_train_norm, validation_data=(X_test_norm, Y_test_norm), epochs=30, batch_size=1, verbose=1, callbacks=[reduce_lr, CustomMetrics()])\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test_norm, Y_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(histSimple.history['loss'])\n",
    "plt.plot(histSimple.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show evolution of learning rate\n",
    "plt.plot(lrHist)\n",
    "plt.title('learning rate evolution')\n",
    "plt.ylabel('learning rate')\n",
    "plt.xlabel('epoch')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to hdf5 file\n",
    "model.save('PolyCombo.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='NoObstacleFirst3IterEndsInOKtf80ZeroFinVelOnly1IterBiggerNetwork.png')\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "model = load_model('PolyCombo.h5')\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('PolyCombo.h5')\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"PolyCombo.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"PolyComboWeights.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "file = h5py.File('PolyComboWeights.h5', 'r')\n",
    "print(list(file.keys()))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model on some inputs and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model('PolyCombo.h5')\n",
    "# This won't randomize as long as seed doesn't change. \n",
    "# (X_train, X_test, Y_train, Y_test) = loadData([\"PolyAstrobeeCoeffs.jld2\"], testRatio = 0.25, fieldNames = [\"train_input_collected\", \"train_output_collected\"])\n",
    "\n",
    "# To be safe, let's do this instead\n",
    "filename = \"PolyComboCoeffsNormalized.jld2\"\n",
    "(X_train_norm, X_test_norm, X_valid_norm, \n",
    " Y_train_norm, Y_test_norm, Y_valid_norm) = loadSplitDataValidTrainTest(filename)\n",
    "\n",
    "filename = \"PolyComboParams.h5\"\n",
    "(xMin_train, yMin_train, xRange_train, yRange_train, \n",
    " xMin_test, yMin_test, xRange_test, yRange_test, \n",
    " xMin_valid, yMin_valid, xRange_valid, yRange_valid) = getDataParamsFromHDF5(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Idx = [0,1,2,3,700]\n",
    "NN_input = np.array(X_train_norm)\n",
    "NN_output_norm = model.predict(NN_input[Idx])\n",
    "NN_output = NN_output_norm*yRange_train + yMin_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Examples\")\n",
    "for NN_idx, train_idx in enumerate(Idx):\n",
    "    true_X_coeff_norm = Y_train_norm[train_idx,0:5]\n",
    "    NN_X_coeff_norm = NN_output_norm[NN_idx,0:5]\n",
    "    plotCoeffsExpectedAndNN(true_X_coeff_norm, NN_X_coeff_norm)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Idx = [61,83,100, 123]\n",
    "NN_input = np.array(X_test_norm)\n",
    "NN_output_norm = model.predict(NN_input[Idx])\n",
    "NN_output = NN_output_norm*yRange_test + yMin_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Examples\")\n",
    "for NN_idx, test_idx in enumerate(Idx):\n",
    "    true_X_coeff_norm = Y_test_norm[test_idx,0:5]\n",
    "    NN_X_coeff_norm = NN_output_norm[NN_idx,0:5]\n",
    "    plotCoeffsExpectedAndNN(true_X_coeff_norm, NN_X_coeff_norm)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Idx = [61,83,100, 101]\n",
    "NN_input = np.array(X_valid_norm)\n",
    "NN_output_norm = model.predict(NN_input[Idx])\n",
    "NN_output = NN_output_norm*yRange_valid + yMin_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Examples\")\n",
    "for NN_idx, valid_idx in enumerate(Idx):\n",
    "    true_X_coeff_norm = Y_valid_norm[valid_idx,0:5]\n",
    "    NN_X_coeff_norm = NN_output_norm[NN_idx,0:5]\n",
    "    plotCoeffsExpectedAndNN(true_X_coeff_norm, NN_X_coeff_norm)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
